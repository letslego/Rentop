{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Rentop Kaggle Competition\n",
    "\n",
    "W207-3 Spring 2017\n",
    "\n",
    "Team members: Stephanie Fan, Boris Kletser, Amitabha Karmakar \n",
    "\n",
    "**Goal:** Use rental listing features to predict interest in rental inquiries.\n",
    "\n",
    "- [Kaggle Competition](https://www.kaggle.com/c/two-sigma-connect-rental-listing-inquiries)\n",
    "- [Notebooks and Code](https://github.com/letslego/Rentop/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Business Understanding\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Problem:** The problem we are trying to solve is two-fold: First, it is to provide feedback to owners and agents on how to optimize listings to generate interest. Secondly, it helps RentHop identify potential issues with listings and fraud. Both of these should help customers better identify relevant listings.\n",
    "\n",
    "**Metrics:** The relevant metric is accurate prediction of high, medium, and low interest. \n",
    "Kaggle will score based on a mutliclass loss. The lower the number, the better.\n",
    "\n",
    "**Delivery:** We will deliver a model that predicts the probability of high, medium, and low interest for a given listing.\n",
    "\n",
    "*Note:* For the purposes of this assignment, we will not be doing analysis of images provided with the competition and will mainly be focusing on using existing features (e.g. text, and values) to try to predict interest level."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Understanding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Sources:**\n",
    "- train.json:  49352 records over 15 columns\n",
    "- test.json:   74659 records over 14 columns\n",
    "\n",
    "Each row is a listing; each column is a feature. The extra column in train.json is the interest level, which we need to predict for test.json."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Existing Features:**\n",
    "    \n",
    "|Feature Type|Columns|Type|Notes|\n",
    "|---|---|---|---|\n",
    "|IDs|building_id|Long string||\n",
    "||listing_id|7 digit num||\n",
    "||manager_id|Long string||\n",
    "|Location|street_address|Text||\n",
    "||display_address|Text||\n",
    "||latitude|Float|New York City only|\n",
    "||longitude|Float|New York City only|\n",
    "|Features|bathrooms|Int|(mean 1.2, sd 0.5)|\n",
    "||bedrooms|Int|(mean 1.5, sd 1.1)|\n",
    "||descriptions|Text||\n",
    "||price|Int||\n",
    "||created|Date|Dates between 2016-04 and 2016-06. Spread throughout weeks, mostly between 1-5am (esp 2am)\n",
    "||photos|List of URLs||\n",
    "|Target Var|interest_level|High/Medium/Low|This is what weâ€™re predicting|\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# code for loading and shape of train/test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### EDA\n",
    "- Distribution of each feature\n",
    "- Missing values\n",
    "- Distribution of target\n",
    "- Relationships between features\n",
    "- Other idiosyncracies?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Transformation and Engineering\n",
    "*[De-duplicating features](https://www.kaggle.com/jxnlco/two-sigma-connect-rental-listing-inquiries/deduplicating-features)*: parses descriptions into consistent rental features (ex: 24-hr concierge) and replaces synonyms with consistent terminology\n",
    "\n",
    "*Text analysis:* Split descriptions into features describing writing style\n",
    "- length of description\n",
    "- number of words\n",
    "- number of capital letters used\n",
    "- number of punctuation marks used\n",
    "- vocabulary richness (use of unique words)\n",
    "\n",
    "*Feature Aggregation & Transformation*: Combine existing features into other features\n",
    "- price per bedroom\n",
    "- price per bathroom\n",
    "- price per room\n",
    "- number of photos per listing\n",
    "- number of claimed rental features\n",
    "- difference between street and display addresses\n",
    "- neighborhoods (based on latitude/longitude)\n",
    "- Multinomial Naive Bayes scoring for description vs interest level\n",
    "- Multinomial Naive Bayes scoring for features vs interest level\n",
    "\n",
    "*Time:* Split features into different time measurements -- does putting up the post at a certain time impact interest?\n",
    "- year (no impact as all rentals were from 2016)\n",
    "- month\n",
    "- day of the month\n",
    "- day of the week\n",
    "- hour\n",
    "- minute\n",
    "- second\n",
    "- time (hr + minutes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Code for feature engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def add_txt_features(orig):\n",
    "    dat = orig\n",
    "    dat.loc[:,'strlen'] = [len(x) for x in dat['description']]\n",
    "    dat.loc[:,'numwords'] = [len(x.split()) for x in dat['description']]\n",
    "    dat.loc[:,'numcaps'] = [sum(1 for c in x if c.upper()) for x in dat['description']]\n",
    "    dat.loc[:,'numpunct'] = [sum(1 for c in x if c in punctuation) for x in dat['description']]\n",
    "    dat.loc[:,'richness'] = [len(set(x)) / (len(x)+0.001) for x in dat['description']] #avoid 0s\n",
    "    return dat\n",
    "\n",
    "def add_price_features(orig):\n",
    "    dat = orig\n",
    "    dat.loc[:,'price_per_bed'] = dat['price'] / (dat['bedrooms']+0.00001)\n",
    "    dat.loc[:,'price_per_bath'] = dat['price'] / (dat['bathrooms']+0.00001)\n",
    "    dat.loc[:,'price_per_room'] = dat['price'] / (dat['bathrooms'] + dat['bedrooms'] +0.00001)\n",
    "    return dat\n",
    "\n",
    "def get_num_photos(orig):\n",
    "    dat = orig\n",
    "    dat.loc[:,'numphotos'] = [len(x) for x in dat['photos'].values]\n",
    "    return dat\n",
    "\n",
    "def add_time_features(orig):\n",
    "    dat = orig\n",
    "    dat.loc[:,\"created2\"] = dat['created'].astype(\"datetime64\");\n",
    "     \n",
    "    dat.loc[:,'year']   = dat['created2'].dt.year\n",
    "    dat.loc[:,'month']  = dat['created2'].dt.month\n",
    "    dat.loc[:,'day']    = dat['created2'].dt.day\n",
    "    dat.loc[:,'weekday']= dat['created2'].dt.dayofweek\n",
    "    dat.loc[:,'hour']   = dat['created2'].dt.hour\n",
    "    dat.loc[:,'minute'] = dat['created2'].dt.minute\n",
    "    dat.loc[:,'second'] = dat['created2'].dt.second\n",
    "    dat.loc[:,'hr_min'] = dat['created2'].dt.hour.multiply(100).add(dat['created2'].dt.minute)\n",
    "    return dat\n",
    "\n",
    "def get_num_features(orig):\n",
    "    dat = orig\n",
    "    dat.loc[:,'numfeatures'] = [len(x) for x in dat['features'].values]\n",
    "    return dat\n",
    "\n",
    "def get_address_dif(orig):\n",
    "    dat = orig\n",
    "    street_addr_len = [len(sa) for sa in dat['street_address']]\n",
    "    display_addr_len = [len(da) for da in dat['display_address']]\n",
    "    dat.loc[:,'addr_dif'] = np.subtract(street_addr_len,display_addr_len)\n",
    "    return dat\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Neighborhoods -- AMIT I didn't standardize these, couldn't get to work at the moment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def remove_outliers(orig):\n",
    "    dat = orig\n",
    "    dat = dat[((dat.latitude - dat.latitude.mean()) / dat.latitude.std()).abs() < 3]\n",
    "    dat = dat[((dat.longitude - dat.longitude.mean()) / dat.longitude.std()).abs() < 3]\n",
    "    return dat\n",
    "\n",
    "def make_neighborhoods(orig, num_clusters):\n",
    "    #returns a km, wth which we can classify other points\n",
    "    dat = orig[['latitude', 'longitude']].copy()\n",
    "    km = KMeans(num_clusters, random_state=1).fit(dat)\n",
    "    return km\n",
    "\n",
    "def fit_neighborhoods(orig, km):\n",
    "    dat = orig[['latitude', 'longitude']].copy()\n",
    "    dat2 = orig\n",
    "    dat2.loc[:,'neighborhood'] = km.predict(dat)\n",
    "    return dat2\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Description and Feature Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def pre_proc(s,\n",
    "              word_length_range=(3,7),\n",
    "              remove_stop_words=True,\n",
    "              scale_capitals=1,\n",
    "              set_to_lower=True,\n",
    "              remove_numbers=False\n",
    "             ):\n",
    "   \n",
    "    s2 = re.sub(ur\"\\p{P}+\",\"\",s) #strip punctuation\n",
    "    s2 = re.sub(ur\"[^\\w ]+\",\" \",s2) #remove punctuation2\n",
    "    s2 = re.sub(ur\"\\_\",\"\",s2) #remove underscores (ignored by w)\n",
    "    \n",
    "    #http://stackoverflow.com/questions/8745821/find-words-with-capital-letters-not-at-start-of-a-sentence-with-regex\n",
    "    #doesn't matter if at start of sentence, often it's the key NP. If a stopword, those get stripped anyway\n",
    "    names = \" \"+\" \".join(re.findall(ur'\\b[A-Z][A-Za-z0-9]*\\b',s2))\n",
    "    for i in range(0,scale_capitals):\n",
    "        s2 = s2 + names\n",
    "        \n",
    "    if set_to_lower:\n",
    "        s2 = s2.lower() #lower case\n",
    "\n",
    "    s2 = re.sub(ur\"\\s+\", \" \",s2) #remove mult spaces (avoids cases with double spaces for look behind)\n",
    "    \n",
    "    if remove_numbers:\n",
    "        s2 = re.sub(ur\"\\d\", \" \",s2) #remove all numbers\n",
    "\n",
    "    truncation_re = ur\"(?<=(\\s\\w{\"+ur\"{}\".format(word_length_range[1])+ur\"}))(\\w*\\s)\"\n",
    "    s2 = re.sub(truncation_re,\"\\1 \",s2) #truncate words > n char\n",
    "\n",
    "    short_elim_re = ur\"\\b\\w{1,\"+ur\"{}\".format(word_length_range[0])+ur\"}\\b\"\n",
    "    s2 = re.sub(short_elim_re, \"\", s2) #removes all words/numbers < n in length\n",
    "    \n",
    "    #remomve stop words\n",
    "    if remove_stop_words:\n",
    "        s2_split = s2.split()\n",
    "        s3_split = s2.split()\n",
    "        for key in s2_split:\n",
    "            if key.lower in stop_words:\n",
    "                s3_split.remove(key)\n",
    "        s2 =' '.join(s3_split)\n",
    "    return s2\n",
    "\n",
    "pre_proc_custom = lambda x: pre_proc(x, \n",
    "                                      word_length_range = (3,8), \n",
    "                                      remove_stop_words = False, \n",
    "                                      scale_capitals = 1, \n",
    "                                      set_to_lower = True,\n",
    "                                      remove_numbers = False\n",
    "                                     )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "mytv = TfidfVectorizer(ngram_range=(1,1), \n",
    "                       analyzer='word', \n",
    "                       preprocessor=pre_proc_custom)\n",
    "mytv.fit_transform(X_train['description'].values)\n",
    "\n",
    "def get_desc_mnb(X, y):    \n",
    "    mytv_dev = TfidfVectorizer(ngram_range=(1,1), \n",
    "                           analyzer='word', \n",
    "                           preprocessor=pre_proc_custom,\n",
    "                           vocabulary=train_words)\n",
    "    \n",
    "    X_dev_words = mytv_dev.fit_transform(X['description'].values) \n",
    "    \n",
    "    mnb = MultinomialNB(alpha = 0.009)\n",
    "    mnb.fit(X_dev_words, y)\n",
    "    return mnb\n",
    "\n",
    "def get_description_scores(X, mnb):\n",
    "    mytv_dev = TfidfVectorizer(ngram_range=(1,1), \n",
    "                           analyzer='word', \n",
    "                           preprocessor=pre_proc_custom,  #set above\n",
    "                           vocabulary=train_words) #also set above\n",
    "    \n",
    "    X_dev_words = mytv_dev.fit_transform(X['description'].values) \n",
    "    \n",
    "    pred_train = mnb.predict_proba(X_dev_words)\n",
    "    \n",
    "    dat = X\n",
    "    dat.loc[:,'desc_1'] = pred_train[:,0]\n",
    "    dat.loc[:,'desc_2'] = pred_train[:,1]\n",
    "    dat.loc[:,'desc_3'] = pred_train[:,2]\n",
    "    return dat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def clean(s):\n",
    "    for i,x in enumerate(s):\n",
    "        x = x.lower()\n",
    "        x = x.strip()\n",
    "        x = x.replace(\"-\", \"\")\n",
    "        x = x.replace(\" \", \"\")\n",
    "        x = x.replace(\"twenty four hour\", \"24\")\n",
    "        x = x.replace(\"24/7\", \"24\")\n",
    "        x = x.replace(\"24hr\", \"24\")\n",
    "        x = x.replace(\"24-hour\", \"24\")\n",
    "        x = x.replace(\"24hour\", \"24\")\n",
    "        x = x.replace(\"24 hour\", \"24\")\n",
    "        x = x.replace(\"common\", \"cm\")\n",
    "        x = x.replace(\"concierge\", \"doorman\")\n",
    "        x = x.replace(\"bicycle\", \"bike\")\n",
    "        x = x.replace(\"private\", \"pv\")\n",
    "        x = x.replace(\"deco\", \"dc\")\n",
    "        x = x.replace(\"decorative\", \"dc\")\n",
    "        x = x.replace(\"onsite\", \"os\")\n",
    "        x = x.replace(\"outdoor\", \"od\")\n",
    "        x = x.replace(\"ss appliances\", \"stainless\")\n",
    "        s[i] = x\n",
    "    return s\n",
    "\n",
    "def clean_features(orig):\n",
    "    dat = orig\n",
    "    dat.loc[:,'cleaned_features'] = [' '.join(clean(f)) for f in dat['features'].values]\n",
    "    return dat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "mytv = TfidfVectorizer(ngram_range=(1,1), \n",
    "                       analyzer='word', \n",
    "                      ) \n",
    "\n",
    "def get_feature_mnb(X, y):\n",
    "    X = clean_features(X)\n",
    "    mytv_dev = TfidfVectorizer(ngram_range=(1,1), \n",
    "                           analyzer='word', \n",
    "                           #preprocessor=pre_proc_custom,  #set above\n",
    "                           vocabulary=train_words) #also set above\n",
    "    \n",
    "    X_dev_words = mytv_dev.fit_transform(X['cleaned_features'].values) \n",
    "    \n",
    "    mnb = MultinomialNB(alpha = 0.009)\n",
    "    mnb.fit(X_dev_words, y)\n",
    "    return mnb\n",
    "\n",
    "def get_feature_scores(X, mnb):\n",
    "    X = clean_features(X)\n",
    "    mytv_dev = TfidfVectorizer(ngram_range=(1,1), \n",
    "                           analyzer='word', \n",
    "                           preprocessor=pre_proc_custom,  #set above\n",
    "                           vocabulary=train_words) #also set above\n",
    "    \n",
    "    X_dev_words = mytv_dev.fit_transform(X['cleaned_features'].values) \n",
    "    \n",
    "    pred_train = mnb.predict_proba(X_dev_words)\n",
    "    #print pred_train\n",
    "    #print mnb.classes_\n",
    "    \n",
    "    dat = X\n",
    "    dat.loc[:,'feat_1'] = pred_train[:,0]\n",
    "    dat.loc[:,'feat_2'] = pred_train[:,1]\n",
    "    dat.loc[:,'feat_3'] = pred_train[:,2]\n",
    "    return dat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Principal component analysis (PCA)\n",
    "Run across training set to see which features are most important. Try running analysis on just those."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pca = PCA()\n",
    "X_transformed = pca.fit_transform(X_train_limited)\n",
    "pca.fit(X_train_limited)\n",
    "pca.explained_variance_ratio_\n",
    "plt.plot(pca.explained_variance_)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Target Transformation\n",
    "Transform target (interest level = high, medium, or low) into ordinal values\n",
    "- high = 3\n",
    "- medium = 2\n",
    "- low = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modeling\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Final model:** Random Forest Classifier\n",
    "\n",
    "*Assumptions:* Features are non-parametric. We picked this method as it is fairly robust and does not require data to be parametric or regularized. In addition, using this method could allow for real-world interpretation of answers in comparison to other models, leading to direct \n",
    "\n",
    "*Regularization:* via PCA [[**AMIT**... would you be able to fix this inside the combining doc?]]\n",
    "    \n",
    "**Other models used:** \n",
    "- linear regression - was too biased toward low\n",
    "- KNN to create neighborhoods\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Splitting the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_df = pd.read_json(\"../input/train.json\")\n",
    "test_df = pd.read_json(\"../input/test.json\")\n",
    "\n",
    "train_df = train_df.set_index('listing_id')\n",
    "test_df = test_df.set_index('listing_id')\n",
    "\n",
    "y = train_df['interest_level']\n",
    "y2 = y.replace({'low':1,'medium':2,'high':3})\n",
    "X = train_df.drop('interest_level', 1)\n",
    "X_train, X_dev, y_train, y_dev = train_test_split(X, y2, test_size=0.2, random_state=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adding in the new features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train = add_txt_features(X_train)\n",
    "X_train = add_price_features(X_train)\n",
    "X_train = get_num_photos(X_train)\n",
    "X_train = add_time_features(X_train)\n",
    "X_train = get_num_features(X_train)\n",
    "X_train = get_address_dif(X_train)\n",
    "\n",
    "neighborhoods = make_neighborhoods(X_train, 50)\n",
    "X_train = fit_neighborhoods(X_train, neighborhoods)\n",
    "\n",
    "my_desc_mnb = get_desc_mnb(X_train, y_train)\n",
    "X_train = get_description_scores(X_train,my_desc_mnb)\n",
    "\n",
    "my_feature_mnb = get_feature_mnb(X_train, y_train)\n",
    "X_train = get_feature_scores(X_train, my_feature_mnb) \n",
    "\n",
    "\n",
    "X_test = add_txt_features(X_test)\n",
    "X_test = add_price_features(X_test)\n",
    "X_test = get_num_photos(X_test)\n",
    "X_test = add_time_features(X_test)\n",
    "X_test = get_num_features(X_test)\n",
    "X_test = get_address_dif(X_test)\n",
    "\n",
    "neighborhoods = make_neighborhoods(X_train, 50)\n",
    "X_test = fit_neighborhoods(X_test, neighborhoods)\n",
    "\n",
    "my_desc_mnb = get_desc_mnb(X_train, y_train)\n",
    "X_test = get_description_scores(X_test,my_desc_mnb)\n",
    "\n",
    "my_feature_mnb = get_feature_mnb(X_train, y_train)\n",
    "X_test = get_feature_scores(X_test, my_feature_mnb) final model\n",
    "\n",
    "\n",
    "#Drop non-numeric features\n",
    "feature_list = ['feat_1','feat_2','feat_3', 'desc_1','desc_2','desc_3',\n",
    "                'year', 'month', 'day', 'weekday', 'hour', 'minute', 'second', 'hr_min',\n",
    "                'numphotos', 'numfeatures','addr_dif','neighborhood',\n",
    "                'price_per_bed','price_per_bath','price_per_room',\n",
    "                'price','bedrooms','bathrooms',\n",
    "                'strlen','numwords','numcaps','numpunct','richness']\n",
    "\n",
    "X_test_limited = X_test[feature_list]\n",
    "print X_test_limited.shape\n",
    "\n",
    "X_train_limited = X_train[feature_list] #index already set\n",
    "\n",
    "rfc = RandomForestClassifier(n_estimators=20, n_jobs=-1) #-1 means use all available cores\n",
    "rfc.fit(X_train_limited, y_train)\n",
    "print X_train_limited.shape\n",
    "labels = rfc.classes_\n",
    "\n",
    "predictions = rfc.predict_proba(X_test_limited)\n",
    "print predictions.shape\n",
    "\n",
    "#reorganize to high mid low\n",
    "temp = X_test_limited\n",
    "temp.loc[:,'high'] = predictions[:,2]\n",
    "temp.loc[:,'medium'] = predictions[:,1]\n",
    "temp.loc[:,'low'] = predictions[:,0]\n",
    "\n",
    "final_table = temp[['high','medium', 'low']]\n",
    "\n",
    "print final_table.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How well does the model perform?\n",
    "Accuracy\n",
    "ROC curves\n",
    "Cross-validation\n",
    "other metrics? performance?\n",
    "AB test results (if any)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluating on training and dev sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def accuracy_report(name, X_train, y_train, X_dev, y_dev):\n",
    "\n",
    "    #RFC\n",
    "    rfc = RandomForestClassifier(n_estimators=20, n_jobs=-1) #-1 means use all available cores\n",
    "    rfc.fit(X_train, y_train)\n",
    "    print('RF Training Accuracy: %.2f%% \\t Test Accuracy: %.2f%%' % (\n",
    "        rfc.score(X_train, y_train)*100,                                                         \n",
    "        rfc.score(X_dev, y_dev)*100))\n",
    "\n",
    "    print ('RFC dev: \\n{}'.format(classification_report(y_dev, rfc.predict(X_dev))))\n",
    "    print itemfreq(rfc.predict(X_dev))\n",
    "\n",
    "    importances = rfc.feature_importances_\n",
    "    features = X_train.columns\n",
    "\n",
    "    sort_indices = np.argsort(importances)[::-1]\n",
    "    sorted_features = []\n",
    "    for i in sort_indices:\n",
    "        sorted_features.append(features[i])\n",
    "\n",
    "    print('\\nfeatures')\n",
    "    print(sorted_features)\n",
    "    print importances[sort_indices]\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Submissions \n",
    "\n",
    "pca_submission02.csv\n",
    "pca PCA PPPPCCCCAAAAA!\n",
    "**1.09215**\n",
    "(PCA on all features)\n",
    "\n",
    "sumbmission004.csv\n",
    "using less features\n",
    "**2.26920**\n",
    "(RFC on top 3 columns of classifier)\n",
    "\n",
    "sumbmission003.csv\n",
    "fixed bugs\n",
    "**1.22052**\n",
    "(All features RFC)\n",
    "\n",
    "sumbmission002.csv\n",
    "rf on mult features\n",
    "**1.23316**\n",
    "(All features RFC)\n",
    "\n",
    "alterntive baseline\n",
    "**0.75440**\n",
    "(Analysis on text -- MNB like hw 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
